{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6a230d",
   "metadata": {},
   "source": [
    "Diego Jose Cantu Martinez A00816015\n",
    "Actividad 3 | Aprendizaje supervisado y no supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27fe9a",
   "metadata": {},
   "source": [
    "Introducción teórica\n",
    "\n",
    "\n",
    "El aprendizaje supervisado es un tipo de aprendizaje  automatico donde se usan conjuntos de datos etiquetados para entrenar el modelo. Esto significa que cada input tiene un output deseado y el objetivo del modelo es devolver la misma salida y aprenderla.\n",
    "\n",
    "Algoritmos representativos:\n",
    "Regresión lineal: Predice un valor continuo basado en una o más variables independientes.\n",
    "\n",
    "Regresión logística: Utilizado para clasificación binaria o multiclase.\n",
    "\n",
    "Árboles de decisión: Algoritmo que divide los datos en subconjuntos basados en condiciones.\n",
    "\n",
    "Random Forest: Conjunto de árboles de decisión que mejora precisión y reduce el sobreajuste.\n",
    "\n",
    "Máquinas de vectores de soporte (SVM): Clasificación mediante hiperplanos óptimos.\n",
    "\n",
    "Redes neuronales: Modelos inspirados en el cerebro humano para aprender relaciones complejas.\n",
    "\n",
    "Algoritmos disponibles en PySpark (MLlib):\n",
    "LinearRegression\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "DecisionTreeClassifier y DecisionTreeRegressor\n",
    "\n",
    "RandomForestClassifier y RandomForestRegressor\n",
    "\n",
    "GBTClassifier (Gradient Boosted Trees)\n",
    "\n",
    "MultilayerPerceptronClassifier (una forma de red neuronal)\n",
    "\n",
    "NaiveBayes\n",
    "\n",
    "El aprendizaje no supervisado se refiere a el tipo de aprendizaje donde los valores de entrada no cuentan con valores de salida predeterminados. El objetivo del modelo es identificar patrones o agrupaciones de manera autonoma.\n",
    "\n",
    "Algoritmos representativos:\n",
    "Clustering (agrupamiento):\n",
    "\n",
    "K-Means: Algoritmo popular que agrupa datos en K clústeres según su similitud.\n",
    "\n",
    "DBSCAN: Agrupa puntos que están densamente conectados.\n",
    "\n",
    "Jerárquico: Crea una jerarquía de clústeres.\n",
    "\n",
    "Reducción de dimensionalidad:\n",
    "\n",
    "PCA (Análisis de Componentes Principales): Reduce la dimensionalidad conservando la mayor varianza posible.\n",
    "\n",
    "t-SNE: Proyección no lineal para visualización.\n",
    "\n",
    "Algoritmos disponibles en PySpark (MLlib):\n",
    "KMeans\n",
    "\n",
    "BisectingKMeans (variante jerárquica)\n",
    "\n",
    "GaussianMixture (modelo de mezcla gaussiana)\n",
    "\n",
    "PCA (para reducción de dimensionalidad)\n",
    "\n",
    "LDA (Latent Dirichlet Allocation, para análisis de temas en texto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac79f3",
   "metadata": {},
   "source": [
    "Selección de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfbb521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "# Descargar archivo Titanic CSV a disco local\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "r = requests.get(url)\n",
    "\n",
    "# Guardar archivo localmente\n",
    "with open(\"titanic.csv\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "df = spark.read.csv(\"titanic.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355b929",
   "metadata": {},
   "source": [
    "Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1a807ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']\n",
    "df = df.select(columns)\n",
    "\n",
    "# Imputar valores nulos en Age y Fare con la media\n",
    "age_mean = df.select(mean(\"Age\")).first()[0]\n",
    "fare_mean = df.select(mean(\"Fare\")).first()[0]\n",
    "\n",
    "df = df.fillna({\"Age\": age_mean, \"Fare\": fare_mean})\n",
    "\n",
    "# Indexar la columna 'Sex'\n",
    "indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Codificación One-Hot\n",
    "encoder = OneHotEncoder(inputCols=[\"SexIndex\"], outputCols=[\"SexVec\"])\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "feature_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'SexVec']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a9cbd",
   "metadata": {},
   "source": [
    "Preparación del conjunto de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5780df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución en el conjunto de entrenamiento:\n",
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       0|  411|\n",
      "|       1|  254|\n",
      "+--------+-----+\n",
      "\n",
      "Distribución en el conjunto de prueba:\n",
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       0|  138|\n",
      "|       1|   88|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dividir por clase para muestreo estratificado\n",
    "df_class_0 = df.filter(df[\"Survived\"] == 0)\n",
    "df_class_1 = df.filter(df[\"Survived\"] == 1)\n",
    "\n",
    "# Fracción de prueba\n",
    "test_frac = 0.3\n",
    "\n",
    "# Separar entrenamiento y prueba para cada clase\n",
    "train_0, test_0 = df_class_0.randomSplit([1 - test_frac, test_frac], seed=42)\n",
    "train_1, test_1 = df_class_1.randomSplit([1 - test_frac, test_frac], seed=42)\n",
    "\n",
    "# Unir ambas clases\n",
    "train_df = train_0.union(train_1)\n",
    "test_df = test_0.union(test_1)\n",
    "\n",
    "\n",
    "# Conteo y proporción en train y test\n",
    "print(\"Distribución en el conjunto de entrenamiento:\")\n",
    "train_df.groupBy(\"Survived\").count().show()\n",
    "\n",
    "print(\"Distribución en el conjunto de prueba:\")\n",
    "test_df.groupBy(\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebca990",
   "metadata": {},
   "source": [
    "Se decidió usar el 70% de los datos para entrenar el modelo y el 30% para probarlo. Esta forma de dividir los datos suele ser un estandar para los modelos supervisados. Con estos porcentajes le da al modelo suficientes datos para que pueda aprender bien y guarda una buena parte para probar si el modelo realmente funciona y no solo se aprendió de memoria los datos del entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3de5d3",
   "metadata": {},
   "source": [
    "Construcción de modelos de aprendizaje supervisado y no supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86770b91",
   "metadata": {},
   "source": [
    "Aprendizaje Supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdcb8403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8375\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Definir modelo\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "\n",
    "# Entrenar modelo\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Predecir en conjunto de prueba\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluar desempeño del modelo\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda978de",
   "metadata": {},
   "source": [
    "Aprendizaje No Supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf3147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.9744\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Definir modelo de clustering con 2 clústeres (porque hay 2 clases en 'Survived')\n",
    "kmeans = KMeans(featuresCol=\"features\", k=2, seed=42)\n",
    "kmeans_model = kmeans.fit(df)\n",
    "\n",
    "# Realizar predicciones (asignación a clúster)\n",
    "clustered = kmeans_model.transform(df)\n",
    "\n",
    "# Evaluar cohesión de los clústeres\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(clustered)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da09393",
   "metadata": {},
   "source": [
    "Interpretaciones Finales "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd0ad1",
   "metadata": {},
   "source": [
    "En caso del modelo de regresion logistica aplicado sobre la muestra se obtubo un AUC de .8375 el AUC deseado es un 1 y algo arriba de .8 indica que el modelo tiene una alta capasidad de distinguir si un pasajero sobrevivio o no. En el caso de k-meeans el indice de siluete de .9744 lo cual respresenta una buena calidad de formacion de clusters Un valor cercano a 1.0 indica que los datos dentro de cada grupo son muy similares entre sí y muy diferentes de los otros grupos. Esto sugiere que las variables seleccionadas para el clustering permitieron diferenciar claramente dos perfiles de pasajeros, sin necesidad de conocer previamente la variable de supervivencia.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
